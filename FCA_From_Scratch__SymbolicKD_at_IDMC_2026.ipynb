{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EgorDudyrev/OntologyPailleur/blob/main/FCA_From_Scratch__SymbolicKD_at_IDMC_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbolic Knowledge Discovery with FCA, TD at IDMC 2026"
      ],
      "metadata": {
        "id": "SesRp_oWHqkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Jupyter notebook provides the first hands-on expirience with Formal Concept Analysis in Python.\n",
        "It is designed in a way so that you can practice converting the math of FCA into Python code, so that you can evaluate the efficiency of the algorithms you've studied (e.g. NextClosure), and so that you can see how FCA output can look on the real data.\n",
        "\n",
        "The notebook is split into 6 (+1) parts:\n",
        "\n",
        "0. Load the data\n",
        "1. Basic FCA operations\n",
        "2. Mining concepts in a smart way (i.e. NextClosure),\n",
        "3. Mining implications in a smart way (through Carpathia-G algorithm),\n",
        "4. Mining association rules (with Luxenburger basis),\n",
        "5. Trying out the real data (the Titanic dataset),\n",
        "6. Pushing the concepts to GraphRAG (a small hint on the project).\n",
        "\n",
        "But keep calm, *you are only asked to write some code in Parts 1, 2, and 3*: the other sections serve demonstration purposes only."
      ],
      "metadata": {
        "id": "1bJ67AQyjj4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0. Load the data"
      ],
      "metadata": {
        "id": "r6ywl7-8x74I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet caspailleur"
      ],
      "metadata": {
        "id": "TwJfNRwdx-Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download one of the classical FCA datasets available at the Formal Context Repository: https://fcarepository.org/"
      ],
      "metadata": {
        "id": "4jSuwWElmQin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from caspailleur.io import from_fca_repo\n",
        "df, metadata = from_fca_repo('newzealand_en.cxt')\n",
        "df"
      ],
      "metadata": {
        "id": "VQguCtaZwgXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the code more structured and more similar to the mathematical definitions, we represent the dataset with this simple `FormalContext` class:"
      ],
      "metadata": {
        "id": "DC5Cc46xme8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class FormalContext(NamedTuple):\n",
        "  objects: set[str]\n",
        "  attributes: set[str]\n",
        "  relations: set[tuple[str, str]]"
      ],
      "metadata": {
        "id": "cbZHRn5s0M72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_to_sets(df) -> FormalContext:\n",
        "  objects = set(df.index)\n",
        "  attributes = set(df.columns)\n",
        "  relations = {(obj, attr) for obj in objects for attr in attributes if df.at[obj, attr]}\n",
        "  return FormalContext(objects, attributes, relations)"
      ],
      "metadata": {
        "id": "EkcSw4U-w1uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = pandas_to_sets(df)\n",
        "print(f\"{context.objects=}\")\n",
        "print(f\"{context.attributes=}\")\n",
        "print(f\"{context.relations=}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K5uexLVGxw5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Basic operations\n",
        "\n",
        "This section introduces two main operations in FCA: the extent and the intent (both are often represented with $\\cdot'$).\n",
        "\n",
        "With these two basic operations (and some minor adjustments), we can already mine concepts and implications. Although such bruteforce algorithms can take a lot of time of the big data."
      ],
      "metadata": {
        "id": "7_Sip_8DyBBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that a Formal Context is a triplet $(G, M, I)$ of the set of objects $G$, the set of attributes $M$, and the incidence relation $I \\subseteq G \\times M$ between objects and attributes."
      ],
      "metadata": {
        "id": "sYr2aMaLy4i9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extent of description $B$ is the (maximal) set of objects described by $B$:\n",
        "$$\\mathtt{extent}(B) = B' = \\{g \\in G \\mid \\forall m \\in M\\ (g, m) \\in I \\}$$"
      ],
      "metadata": {
        "id": "zoQi8zA2yKD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extent(description: set[str], context: FormalContext) -> set[str]:\n",
        "  # TODO: Implement the extent operation\n",
        "  # HINT: Pythonic set-comprehension looks A LOT like the mathematical set-builder notation above\n",
        "  return ...\n",
        "\n",
        "extent({'Jet Boating', 'Wildwater Rafting'}, context)"
      ],
      "metadata": {
        "id": "qryP3IZPyCNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intent of a subset of objects $A$ is the (maximal) set of attributes that describe $A$:\n",
        "$$\\mathtt{intent}(A) = A' = \\{m \\in M \\mid \\forall g \\in A, (g,m) \\in I \\}$$"
      ],
      "metadata": {
        "id": "seZq25I91Qhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intent(objects: set[str], context: FormalContext) -> set[str]:\n",
        "  # TODO: Implement the intent operation\n",
        "  return ...\n",
        "\n",
        "intent({'Queenstown', 'Wanaka'}, context)"
      ],
      "metadata": {
        "id": "MvIat1eB1jjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us import the `powerset` function from `caspailleur` package in order to iterate through the powerset of a set (i.e. iterate through all the subsets of some given set)"
      ],
      "metadata": {
        "id": "_c0UBJ-f3EsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from caspailleur.base_functions import powerset\n",
        "\n",
        "list(powerset({'a', 'b', 'c'}))"
      ],
      "metadata": {
        "id": "46sww4sxn4l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement the bruteforce algorithm for mining formal concepts: it goes through all the (exponential amount of) possible descriptions, and computes their extents and intents."
      ],
      "metadata": {
        "id": "-6qiDOWJolXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FormalConcept(NamedTuple):\n",
        "  extent: set[str]  # all objects covered by the concept\n",
        "  intent: set[str]  # all attributes covered by the concept\n",
        "  support: float = None  # the number of objects in the extent (optional)\n",
        "\n",
        "  def __hash__(self) -> int:\n",
        "    return hash((frozenset(self.extent), frozenset(self.intent)))"
      ],
      "metadata": {
        "id": "T2GQIgWFwy_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mine_concepts_bruteforce(context: FormalContext) -> set[FormalConcept]:\n",
        "  concepts = set()\n",
        "  for description in powerset(context.attributes):\n",
        "    # ToDo: Finish the Bruteforce algorithm for mining concepts.\n",
        "    # Compute the extent and the intent of a given description and form them into a FormalConcept\n",
        "    extent_ = ...\n",
        "    intent_ = ...\n",
        "    concepts.add(FormalConcept(extent_, intent_, len(extent_)/len(context.objects)))\n",
        "  return concepts"
      ],
      "metadata": {
        "id": "zE5szz-b4V6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "concepts = mine_concepts_bruteforce(context)\n",
        "pd.DataFrame(concepts).sort_values('support', ascending=False)"
      ],
      "metadata": {
        "id": "2mMMqsjT4_6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bruteforce generation of implications. Again, we pass through every subset of attributes and use it to make an implication \"subset_of_attributes => its_closure\". And then we filter out some of the \"reduntant\" implications to get the Proper Premise basis of implications."
      ],
      "metadata": {
        "id": "9hZbT3Xv7KF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Implication(NamedTuple):\n",
        "  premise: set[str]\n",
        "  conclusion: set[str]\n",
        "  support: float = None\n",
        "\n",
        "  def __hash__(self) -> int:\n",
        "    return hash((frozenset(self.premise), frozenset(self.conclusion)))"
      ],
      "metadata": {
        "id": "uCQggp3zxC9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mine_implications_bruteforce(context: FormalContext) -> set[Implication]:\n",
        "  implications = set()\n",
        "  for description in powerset(context.attributes):\n",
        "    # ToDo: Compute the `extent_` and the `intent_` of the description to form an implication \"description => intent_\"\n",
        "    extent_ = ...\n",
        "    intent_ = ...\n",
        "\n",
        "    # And now we filter some \"obvious\" and \"reduntant\" implications,\n",
        "    # so that the final set of implications would make a Proper Premise basis (aka Canonical Direct basis)\n",
        "\n",
        "    # if implication is obvious: i.e. description => description, then we do not want to see it\n",
        "    if description == intent_:\n",
        "      continue\n",
        "\n",
        "    # if implication can be expressed by combining smaller implications, then we do not want to see it\n",
        "    subdescriptions = {description - {attr} for attr in description}\n",
        "    subintent = set(description)\n",
        "    for subdescription in subdescriptions:\n",
        "      subintent |= intent(extent(subdescription, context), context)\n",
        "    if subintent == intent_:\n",
        "      continue\n",
        "\n",
        "    # we remove duplicating attributes from the conclusion, so that the implication would look more concise\n",
        "    conclusion = intent_ - description\n",
        "\n",
        "    support = len(extent_)/ len(context.objects)\n",
        "    implications.add(Implication(description, conclusion, support))\n",
        "  return implications"
      ],
      "metadata": {
        "id": "kt9qcN41xP6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "implications = mine_implications_bruteforce(context)\n",
        "pd.DataFrame(implications).sort_values('support', ascending=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OJLJzQ0U52ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Mining concepts in a smart way\n",
        "\n",
        "The bruteforce way of iterating through all subsets of attributes might be very expensive when the data is big. So it is better to use some advanced algorithms. For example, NextClosure."
      ],
      "metadata": {
        "id": "794OWRHs7rh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_lectically_smaller(description_left: set[str], description_right: set[str], index: int, attributes_list: list[str]) -> bool:\n",
        "  # ToDo: Implement lectic order comparison for the next closure algorithm\n",
        "  # The formula can be found on Slide 63 of the class where \"description_left\" is reffered to as \"A\" and \"description_right\" is reffered to as \"B\".\n",
        "  # Note that the \"index\" here is a number, and not an attributes (as on the slides)\n",
        "  ...\n",
        "  return ...\n",
        "\n",
        "\n",
        "def next_closure(intent_: set[str], attributes_list: list[str], context: FormalContext) -> frozenset[str]:\n",
        "  # ToDo: Implement the NextClosure algorithm from Slide 65 of the class\n",
        "  for i in reversed(range(len(attributes_list))):\n",
        "    ...\n",
        "  return context.attributes"
      ],
      "metadata": {
        "id": "ewJf-M9kZlZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_closure(context: FormalContext) -> set[FormalConcept]:\n",
        "  # AllClosure function. It is quite simple, so there is nothing to implement.\n",
        "  concepts = set()\n",
        "\n",
        "  attributes_list = sorted(context.attributes)\n",
        "  intent_ = intent(extent(set(), context), context)\n",
        "  while True:\n",
        "    extent_ = extent(intent_, context)\n",
        "    concepts.add(FormalConcept(extent_, intent_, len(extent_)/len(context.objects)))\n",
        "\n",
        "    if intent_ == context.attributes:\n",
        "      break\n",
        "\n",
        "    intent_ = next_closure(intent_, attributes_list, context)\n",
        "\n",
        "  return concepts"
      ],
      "metadata": {
        "id": "g5NY8DdCY8UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "concepts = all_closure(context)\n",
        "pd.DataFrame(concepts).sort_values('support', ascending=False)"
      ],
      "metadata": {
        "id": "gkBk0XLO2ZEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that the bruteforce and the smart way give the same output"
      ],
      "metadata": {
        "id": "RDCzk9X6sEto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert all_closure(context) == mine_concepts_bruteforce(context)"
      ],
      "metadata": {
        "id": "1M3Aw1Mt2iTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit mine_concepts_bruteforce(context)\n",
        "%timeit all_closure(context)"
      ],
      "metadata": {
        "id": "_ORebM1CBW1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Mining implications in a smart(er) way\n",
        "\n",
        "In this Part we mine Proper Premise basis of implications in a smarter way.\n",
        "\n",
        "Recall that a **proper premise** is defined as follows:\n",
        "$$ P \\subseteq M \\text{ is a proper premise} \\iff P \\cup \\bigcup_{m \\in P} (P \\setminus \\{m\\})'' \\neq P''.$$\n",
        "\n",
        "The mathematical definition implies an important property:\n",
        "$$P \\cup \\bigcup_{m \\in P} (P \\setminus \\{m\\})'' \\neq P'' \\implies (P \\setminus \\{m\\})'' \\neq P'', \\forall m \\in P.$$\n",
        "To put it into words:\n",
        "$$P \\text{ is a proper premise} \\implies P \\text{ is a minimal generator}.$$\n",
        "\n",
        "And **minimal generator** is a description, such that its every subset of attributes describes more objects. So, it is impossible to make a minimal generator smaller and still describe the same objects.\n",
        "\n",
        "To conclude, one way to mine proper premises would be to 1) mine all the minimal generators, 2) filter out minimal generators that are not proper premises."
      ],
      "metadata": {
        "id": "1LOOrjl8DvDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is one of a simple-yet-efficient algorithm for mining minimal generators called Carpathia-G.\n",
        "\n",
        "The algorithm uses one property of a minimal generator: $$\\text{every subset of a minimal generator is a minimal generator}.$$\n",
        "\n",
        "That is, if there is some description s.t. one of its subsets is not a minimal generator, then the description itself is not a minimal generator."
      ],
      "metadata": {
        "id": "XJK41_T04jmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carpathia_g(context: FormalContext) -> list[set[str]]:\n",
        "  mingens: set[frozenset[str]] = set()\n",
        "  attributes_list = list(context.attributes)\n",
        "\n",
        "  # The algorithm goes through various subsets of attributes, starting from the smallest one (the empty subset) and incrementally going to more and more complex descriptions\n",
        "  queue = [frozenset()]\n",
        "  while queue:\n",
        "    description = queue.pop(0)\n",
        "    subdescriptions = {description - {attr} for attr in description}\n",
        "    # ToDo: Implement Test #1: if some subdescriptions are not in `mingens` set, the `description` is not a minimal generator\n",
        "    if ...:\n",
        "      continue  # not a minimal generator, so go to another description in the queue\n",
        "\n",
        "    # ToDo: Implement Test #2: if some subdescription is describes the same extent, then `description` is not a minimal generator\n",
        "    extent_ = extent(description, context)\n",
        "    if ...:\n",
        "      continue  # not a minimal generator, so go to another description in the queue\n",
        "\n",
        "    # All tests passed, so the description is a minimal generator\n",
        "    mingens.add(description)\n",
        "\n",
        "    # If the two tests were successfull, then we can try to add one more attributes to the description\n",
        "    max_idx = max(attributes_list.index(attr) for attr in description) if description else -1\n",
        "    next_descriptions = [description | {m} for m in attributes_list[max_idx+1:]]\n",
        "    queue.extend(next_descriptions)\n",
        "\n",
        "  return [frozenset(mingen) for mingen in mingens]"
      ],
      "metadata": {
        "id": "oxo-ltk5UFtB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3fb9800b-5fb6-4d15-e764-680ad9c78b8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FormalContext' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3569949611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcarpathia_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFormalContext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mmingens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mattributes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# The algorithm goes through various subsets of attributes, starting from the smallest one (the empty subset) and incrementally going to more and more complex descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FormalContext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know how to iterate minimal generators (using Carpathia-G), now we just should identify the minimal generators that are also proper premises. You can do that by following the definition of the proper premise above."
      ],
      "metadata": {
        "id": "Y7PibPz-wAPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_proper_premise(min_generator: frozenset[str], context: FormalContext) -> bool:\n",
        "  # ToDo: Implement the definition of a proper premise (the first formula in Part 3) to test if a `min_generator` is a proper premise,\n",
        "  ..."
      ],
      "metadata": {
        "id": "8AtNycBvWGGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "9f42337f-c665-4b5d-edfe-7a3ae44cdcdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FormalContext' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3907831026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_proper_premise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_generator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFormalContext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# ToDo: Implement the definition of a proper premise (the first formula in Part 3) to test if a `min_generator` is a proper premise,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FormalContext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mine_implications_smart(context: FormalContext) -> set[Implication]:\n",
        "  implications: set[Implication] = set()\n",
        "\n",
        "  attributes_list = list(context.attributes)\n",
        "\n",
        "  # the algorithm is simple: go through every minimal generator, if it is a proper premise, then make it an Implication\n",
        "  for mingen in carpathia_g(context):\n",
        "    if is_proper_premise(mingen, context):\n",
        "      extent_ = extent(mingen, context)\n",
        "      conclusion = intent(extent_, context) - mingen\n",
        "      support = len(extent_) / len(context.objects)\n",
        "      implications.add(Implication(set(mingen), conclusion, support))\n",
        "\n",
        "  return implications"
      ],
      "metadata": {
        "id": "EX3UmHpxDzRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that the bruteforce and the smart way give the smae output"
      ],
      "metadata": {
        "id": "WtKW0a80wfZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert mine_implications_smart(context) == mine_implications_bruteforce(context)"
      ],
      "metadata": {
        "id": "yJRduKBC5adG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit mine_implications_bruteforce(context)\n",
        "%timeit mine_implications_smart(context)"
      ],
      "metadata": {
        "id": "wu0Cbpey9iOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4. Association rules\n",
        "\n",
        "Implications can often be too \"strict\" to use them on real data. One of the solutions is to mine **association rules** that are just like the implications rules but with varying confidence."
      ],
      "metadata": {
        "id": "cEsjvRqR-idi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AssociationRule(NamedTuple):\n",
        "  premise: set[str]\n",
        "  conclusion: set[str]\n",
        "  confidence: float = None\n",
        "  support: float = None\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash((frozenset(self.premise), frozenset(self.conclusion)))"
      ],
      "metadata": {
        "id": "uZN2J5-n-uxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the classical way to compute the basis of association rules is to use so-called Luxenburger basis. It finds all pairs of neighbouring concepts in a concept lattice and makes an association rule that the intent of the greater concept associates to the intent of the smaller concept."
      ],
      "metadata": {
        "id": "ZXeFXkTIw3Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def luxenburger_basis(context: FormalContext) -> set[AssociationRule]:\n",
        "  concepts = all_closure(context)\n",
        "\n",
        "  associations = set()\n",
        "\n",
        "  for concept in concepts:\n",
        "    subconcepts = {other for other in concepts if other.extent < concept.extent}\n",
        "    top_subconcepts = {\n",
        "        subconcept for subconcept in subconcepts\n",
        "        if not any(subconcept.extent < higher_sub.extent for higher_sub in subconcepts)\n",
        "    }\n",
        "\n",
        "    for subconcept in top_subconcepts:\n",
        "      confidence = len(subconcept.extent)/len(concept.extent) if subconcept.extent else 1\n",
        "      support = len(subconcept.extent) / len(context.objects)\n",
        "\n",
        "      conclusion = subconcept.intent - concept.intent\n",
        "      associations.add(AssociationRule(concept.intent, conclusion, confidence, support))\n",
        "  return associations"
      ],
      "metadata": {
        "id": "1qYY6bje-hag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(luxenburger_basis(context))"
      ],
      "metadata": {
        "id": "mKNJhWkW_pIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5. The Real data\n",
        "\n",
        "Let us try to run FCA algorithms on some real data, e.g. the Titanic one.\n",
        "All the code here is already written but you are invited to generate new binary attributes on this complex data.\n",
        "\n",
        "There are two important questions: 1) what are some interesting dependancies on the Titanic data, and 2) at what point the running time of FCA algorithms will explode exponentially."
      ],
      "metadata": {
        "id": "Ocv3NAgSD1UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv', index_col=0)\n",
        "titanic_df.index = 'Passenger '+titanic_df.index.astype(str)\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "6-BHDNgOD3Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some of possible binary attributes. You can always propose your own attributes.\n",
        "titanic_bin = {\n",
        "    'Survived': titanic_df['Survived'] == 1,\n",
        "    'FirstClass': titanic_df['Pclass'] == 1,\n",
        "    'French': titanic_df['Embarked'] == 'C',  # 'C' means 'Cherbourg'\n",
        "    'Adult': titanic_df['Age'] >= 18,\n",
        "    'Child': titanic_df['Age'] < 18,\n",
        "    'Male': titanic_df['Sex'] == 'male',\n",
        "    'Female': titanic_df['Sex'] == 'female',\n",
        "}\n",
        "titanic_bin = pd.DataFrame(titanic_bin)\n",
        "titanic_bin.head()"
      ],
      "metadata": {
        "id": "LufF3Wl8CPl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_context = pandas_to_sets(titanic_bin)"
      ],
      "metadata": {
        "id": "NFRSmDqSDFYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert mine_concepts_bruteforce(titanic_context) == all_closure(titanic_context)\n",
        "%timeit mine_concepts_bruteforce(titanic_context)\n",
        "%timeit all_closure(titanic_context)"
      ],
      "metadata": {
        "id": "0Ngm6VyJE1yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that sometimes advanced algorithms can work slower than the bruteforce ones: that happens when the data is too simple.\n",
        "\n",
        "Another way to make the algorithms faster is by using appropriate data structures. In particular, binary datasets can be represented as a list of bitmasks (or `bitarray`s). This way, the algorithms might work thousand times faster."
      ],
      "metadata": {
        "id": "nGjFhrTYyNtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concepts = all_closure(titanic_context)\n",
        "pd.DataFrame(concepts).sort_values('support', ascending=False)"
      ],
      "metadata": {
        "id": "BxFHe_iRDNA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert mine_implications_smart(titanic_context) == mine_implications_bruteforce(titanic_context)\n",
        "%timeit mine_implications_bruteforce(titanic_context)\n",
        "%timeit mine_implications_smart(titanic_context)"
      ],
      "metadata": {
        "id": "ifgTVOUGGcA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "implications = mine_implications_smart(titanic_context)\n",
        "pd.DataFrame(implications).sort_values('support', ascending=False)"
      ],
      "metadata": {
        "id": "N53pDAC9DrMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "associations = luxenburger_basis(titanic_context)\n",
        "association_df = pd.DataFrame(associations).sort_values(['support', 'confidence'], ascending=False).reset_index(drop=True)\n",
        "association_df = association_df[(association_df['confidence']>0.5)&(association_df['support']>=0.1)]\n",
        "association_df"
      ],
      "metadata": {
        "id": "oykYMiVvETUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6. Integration with GraphRAG and Ontologies\n",
        "\n",
        "And here is some hint on what will happen during the second half of the course and during the project work.\n",
        "\n",
        "Except that we will learn how to find concepts, implications, and associations on complex data (i.e. numerical, categorical, graphs, etc.), so the binarisation step will become obsolete."
      ],
      "metadata": {
        "id": "7RAYlYl0FzoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6.1. Create ontology"
      ],
      "metadata": {
        "id": "cxpUUsUe99dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet git+https://github.com/smartFCA/OntologyPailleur"
      ],
      "metadata": {
        "id": "yziOYI0V92sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should change the concepts representation a little bit. In `OntologyPailleur` package every formal concept is a triple of (extent, intent, minimal_generators).\n"
      ],
      "metadata": {
        "id": "Ky7PpoUFzHUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mingens_per_closure = {frozenset(implication.premise | implication.conclusion): [] for implication in implications}\n",
        "for implication in implications:\n",
        "  closure = frozenset(implication.premise | implication.conclusion)\n",
        "  mingens_per_closure[closure].append(implication.premise)"
      ],
      "metadata": {
        "id": "KhmlGyx0BPk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ontopailleur import formal_ontology_constructor as foc\n",
        "concepts_to_save = [foc.FormalConcept(extent(intent, titanic_context), intent, mingens) for intent, mingens in mingens_per_closure.items()]"
      ],
      "metadata": {
        "id": "LWWxa-PeAyqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `rdflib` to create knowledge graphs in Turtle format (.ttl) within Python.\n",
        "\n",
        "Every node of a knowledge graph (including the ones representing objects $G$ and attributes $M$), should be represented with some `rdflib.URIRef(node_name)`. The name of the node should start with a prefix (that can be done via `rdflib.Namespace`), and should contain neither empty spaces nor special symbols.\n",
        "\n",
        "The code below compute automatic URIRef's for each object and attribute, but you might need to change the code if you have some particular names of objects and attributes."
      ],
      "metadata": {
        "id": "eDbONNSvzdVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdflib\n",
        "NSpace = rdflib.Namespace('http://idmc.univ-lorraine.fr/td_fca#')\n",
        "obj_to_refs = {obj: rdflib.URIRef(NSpace[obj.replace(' ', '_').capitalize()]) for obj in titanic_context.objects}\n",
        "attr_to_refs = {attr: rdflib.URIRef(NSpace[attr.replace(' ', '_').capitalize()]) for attr in titanic_context.attributes}\n",
        "\n",
        "attr_to_refs"
      ],
      "metadata": {
        "id": "0I568n2hFAgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kgraph = foc.construct_ontology(NSpace, prefix='tdfca', objects_to_refs=obj_to_refs, attributes_to_refs=attr_to_refs, incidence_relation=titanic_context.relations, concepts=concepts_to_save)\n",
        "print(\"First 5 RDF triplets of the knowledge graph\")\n",
        "list(kgraph.triples((None, None, None)))[:5]"
      ],
      "metadata": {
        "id": "zJaUD-bOFFwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kgraph.serialize('kgraph.ttl')"
      ],
      "metadata": {
        "id": "-7ejXBXqFo-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the saved graph and open it in Protégé system as an ontology."
      ],
      "metadata": {
        "id": "MVXN24ym0oO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6.2 Running GraphRAG\n",
        "\n",
        "Now we want to put the obtained Knowledge Graph into a GraphRAG pipeline. To do so, we should download some more packages and neural networks."
      ],
      "metadata": {
        "id": "gzrN7M3oFtxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet networkx matplotlib torch transformers sentence-transformers lmdb -q\n",
        "!pip install --quiet tf_keras"
      ],
      "metadata": {
        "id": "2QzB4dglGPgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ontopailleur import graphrag\n",
        "\n",
        "EMBEDDING_MODEL_NAME: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL_NAME: str = 'Qwen/Qwen2-1.5B-Instruct'  # 'Qwen/Qwen2-0.5B-Instruct'\n",
        "LMDB_PATH: str = './embeddings_local'\n",
        "GRAPH_PATH = 'kgraph.ttl'"
      ],
      "metadata": {
        "id": "wAI7SZb1FwAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = graphrag.compute_embeddings(GRAPH_PATH, EMBEDDING_MODEL_NAME, LMDB_PATH)"
      ],
      "metadata": {
        "id": "LdROUg4iGBvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grag = graphrag.GraphRAG(GRAPH_PATH, LMDB_PATH, embedding_model)"
      ],
      "metadata": {
        "id": "CU7oWwt2GCJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model = graphrag.initialise_llm(LLM_MODEL_NAME)"
      ],
      "metadata": {
        "id": "r1_LcsX-Gm4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can ask use GraphRAG system for asking questions about the data."
      ],
      "metadata": {
        "id": "YRXBrCpX1FTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graphrag.graphrag_query(grag, tokenizer, model, 'What describes Passenger 520?', hops=1)"
      ],
      "metadata": {
        "id": "e0V2srCfGwU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}